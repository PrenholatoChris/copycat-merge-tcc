{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9634dabf-f0f8-4770-9ea9-064ea132f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copycat_framework.copycat import Copycat, Problem, Config\n",
    "\n",
    "from sys import exit\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "from copycat_framework.copycat.utils import set_seeds\n",
    "\n",
    "'''Quero fazer o seguinte:\n",
    "Treinar 1 oraculo com bae no dataset MNIST e após isso realizar o ataque de extração para o copycat.\n",
    "Depois quero treinar outro oraculo com o SVHN e então realizar o ataque de extração para o mesmo modelo copycat.\n",
    "Assim o copycat terá 2 conhecimento de 2 domínios diferentes.'''\n",
    "''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6fa4c9-2f39-4933-9acb-c64fb4b8142e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Option Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1e6f0d2-1ddd-4027-b0f8-f255e5c483a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self,\n",
    "                 #required parameters:\n",
    "                 problem_name, oracle_filename, copycat_filename, finetune_filename,\n",
    "                 train_oracle,\n",
    "                 train_copycat, label_copycat_dataset,\n",
    "                 train_finetune, label_finetune_dataset,\n",
    "                 only_print_reports,\n",
    "                 #general configutation:\n",
    "                 config_file=None,\n",
    "                 validation_step=None,\n",
    "                 save_snapshot=None,\n",
    "                 #oracle:\n",
    "                 oracle_arch=None, oracle_max_epochs=None, oracle_batch_size=None, oracle_lr=None,\n",
    "                 oracle_gamma=None, oracle_dataset_root=None, oracle_resume_filename=None,\n",
    "                 #copycat:\n",
    "                 copycat_arch=None, copycat_max_epochs=None, copycat_batch_size=None, copycat_lr=None,\n",
    "                 copycat_gamma=None, copycat_dataset_root=None, copycat_balance_dataset=None, copycat_resume_filename=None,\n",
    "                 #finetune:\n",
    "                 finetune_max_epochs=None, finetune_batch_size=None, finetune_lr=None,\n",
    "                 finetune_gamma=None, finetune_dataset_root=None, finetune_balance_dataset=None, finetune_resume_filename=None,\n",
    "                 #seed\n",
    "                 seed=None):\n",
    "        #required:\n",
    "        self.problem_name = problem_name\n",
    "        self.oracle_filename = oracle_filename\n",
    "        self.copycat_filename = copycat_filename\n",
    "        self.finetune_filename = finetune_filename\n",
    "        self.train_oracle = train_oracle\n",
    "        self.train_copycat = train_copycat\n",
    "        self.label_copycat_dataset = label_copycat_dataset\n",
    "        self.train_finetune = train_finetune\n",
    "        self.label_finetune_dataset = label_finetune_dataset\n",
    "        self.only_print_reports = only_print_reports\n",
    "        #genearal configuration:\n",
    "        self.config = None\n",
    "        self.config_file = config_file\n",
    "        self.validation_step = validation_step\n",
    "        self.save_snapshot = save_snapshot\n",
    "        self.problems = self.get_problem_names()\n",
    "        #oracle:\n",
    "        self.oracle_arch=oracle_arch\n",
    "        self.oracle_resume_filename = oracle_resume_filename\n",
    "        self.oracle_max_epochs = oracle_max_epochs\n",
    "        self.oracle_batch_size = oracle_batch_size\n",
    "        self.oracle_lr = oracle_lr\n",
    "        self.oracle_gamma = oracle_gamma\n",
    "        self.oracle_dataset_root = oracle_dataset_root\n",
    "        #copycat:\n",
    "        self.copycat_arch = copycat_arch\n",
    "        self.copycat_resume_filename = copycat_resume_filename\n",
    "        self.copycat_max_epochs = copycat_max_epochs\n",
    "        self.copycat_batch_size = copycat_batch_size\n",
    "        self.copycat_lr = copycat_lr\n",
    "        self.copycat_gamma = copycat_gamma\n",
    "        self.copycat_dataset_root = copycat_dataset_root\n",
    "        self.copycat_balance_dataset = copycat_balance_dataset\n",
    "        #finetune:\n",
    "        self.finetune_resume_filename = finetune_resume_filename\n",
    "        self.finetune_max_epochs = finetune_max_epochs\n",
    "        self.finetune_batch_size = finetune_batch_size\n",
    "        self.finetune_lr = finetune_lr\n",
    "        self.finetune_gamma = finetune_gamma\n",
    "        self.finetune_dataset_root = finetune_dataset_root\n",
    "        self.finetune_balance_dataset = finetune_balance_dataset\n",
    "        #seed\n",
    "        self.seed = seed\n",
    "\n",
    "    def __load_config(self):\n",
    "        if self.config is None:\n",
    "            self.config = Config(self.config_file)\n",
    "        return self.config\n",
    "\n",
    "    def parse_value(self, attr_name):\n",
    "        self.__load_config()\n",
    "        if attr_name.startswith('oracle'):\n",
    "            opts = self.config.get_oracle_options(self.problem_name)\n",
    "            config_key = attr_name.replace('oracle_','')\n",
    "        elif attr_name.startswith('copycat'):\n",
    "            opts = self.config.get_copycat_options(self.problem_name)\n",
    "            config_key = attr_name.replace('copycat_','')\n",
    "        elif attr_name.startswith('finetune'):\n",
    "            opts = self.config.get_finetune_options(self.problem_name)\n",
    "            config_key = attr_name.replace('finetune_','')\n",
    "        else:\n",
    "            opts = self.config.get_general_options()\n",
    "            config_key = attr_name\n",
    "        try:\n",
    "            local_value = getattr(self, attr_name)\n",
    "            config_value = opts[config_key] if config_key in opts else None\n",
    "            return local_value if local_value is not None else config_value\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def get_db_name(self, model, db_name):\n",
    "        self.__load_config()\n",
    "        name = self.config.get_value(self.problem_name, db_name, model=model)\n",
    "        if name is not None:\n",
    "            aux = self.config.get_value(self.problem_name, 'data', model=model)['datasets']\n",
    "            if name in aux:\n",
    "                return name, aux[name]\n",
    "            else:\n",
    "                return name, None\n",
    "        return None, None\n",
    "\n",
    "    def get_problem_names(self):\n",
    "        return Config(self.config_file).get_problem_names()\n",
    "\n",
    "    def __get_repr_oracle(self):\n",
    "        fmt = f\"  Oracle:\\n\"\n",
    "        if self.train_oracle:\n",
    "            fmt+= f\"     Model filename: '{self.oracle_filename}'\\n\"\n",
    "            fmt+= f\"     Model arch: {self.oracle_arch}\\n\"\n",
    "        if self.oracle_resume_filename is not None:\n",
    "            fmt+= f\"     Resume filename: '{self.oracle_resume_filename}'\\n\"\n",
    "        # max epochs\n",
    "        if self.train_oracle and not self.only_print_reports:\n",
    "            fmt+= f\"     Maximum training epochs: {self.parse_value('oracle_max_epochs')}\\n\"\n",
    "            # batch size\n",
    "            fmt+= f\"     Batch size: {self.parse_value('oracle_batch_size')}\\n\"\n",
    "            # lr\n",
    "            fmt+= f\"     Learning Rate: {self.parse_value('oracle_lr')}\\n\"\n",
    "            # gamma\n",
    "            fmt+= f\"     Gamma: {self.parse_value('oracle_gamma')}\\n\"\n",
    "            db = self.get_db_name(model='oracle', db_name='db_train')\n",
    "            if db[0] is not None:\n",
    "                fmt+= f\"     Dataset: {db[0]}\"\n",
    "                if db[1] is not None:\n",
    "                    fmt+= f\" ('{db[1]}')\"\n",
    "                fmt+= '\\n'\n",
    "            db_test = self.get_db_name(model='oracle', db_name='db_test')\n",
    "            if db_test[0] is not None:\n",
    "                fmt+= f\"     Test dataset: {db_test[0]}\"\n",
    "                if db_test[1] is not None:\n",
    "                    fmt+= f\" ('{db_test[1]}')\"\n",
    "                fmt+= '\\n'\n",
    "        else:\n",
    "            fmt+= f\"     It will NOT be trained.\\n\"\n",
    "        db_root = self.parse_value('oracle_dataset_root')\n",
    "        if db_root != '': fmt+= f\"     Dataset root: '{db_root}'\\n\"\n",
    "        return fmt\n",
    "\n",
    "    def __get_repr_copycat(self):\n",
    "        fmt = f\"  Copycat:\\n\"\n",
    "        if self.train_copycat:\n",
    "            fmt+= f\"     Model filename: '{self.copycat_filename}'\\n\"\n",
    "            fmt+= f\"     Model arch: {self.copycat_arch}\\n\"\n",
    "        if self.copycat_resume_filename is not None:\n",
    "            fmt+= f\"     Resume filename: '{self.copycat_resume_filename}'\\n\"\n",
    "        if self.train_copycat and not self.only_print_reports:\n",
    "            fmt+= f\"     Maximum training epochs: {self.parse_value('copycat_max_epochs')}\\n\"\n",
    "            # batch size\n",
    "            fmt+= f\"     Batch size: {self.parse_value('copycat_batch_size')}\\n\"\n",
    "            # lr\n",
    "            fmt+= f\"     Learning Rate: {self.parse_value('copycat_lr')}\\n\"\n",
    "            # gamma\n",
    "            fmt+= f\"     Gamma: {self.parse_value('copycat_gamma')}\\n\"\n",
    "            db = self.get_db_name(model='copycat', db_name='db_train')\n",
    "            if db[0] is not None:\n",
    "                fmt+= f\"     Dataset: {db[0]}\"\n",
    "                if db[1] is not None:\n",
    "                    fmt+= f\" ('{db[1]}')\"\n",
    "                fmt+= '\\n'\n",
    "            db_test = self.get_db_name(model='copycat', db_name='db_test')\n",
    "            if db_test[0] is not None:\n",
    "                fmt+= f\"     Test dataset: {db_test[0]}\"\n",
    "                if db_test[1] is not None:\n",
    "                    fmt+= f\" ('{db_test[1]}')\"\n",
    "                fmt+= '\\n'\n",
    "            fmt+= f\"     The training dataset will {'' if self.parse_value('copycat_balance_dataset') else 'NOT '}be balanced.\\n\"\n",
    "            fmt+= f\"     The training dataset will {'' if self.label_copycat_dataset else 'NOT '}be labeled by the Oracle Model.\\n\"\n",
    "        else:\n",
    "            fmt+= f\"     It will NOT be trained.\\n\"\n",
    "        db_root = self.parse_value('copycat_dataset_root')\n",
    "        if db_root != '': fmt+= f\"     Dataset root: '{db_root}'\\n\"\n",
    "        return fmt\n",
    "\n",
    "    def __get_repr_finetune(self):\n",
    "        fmt = f\"  Copycat Finetuning:\\n\"\n",
    "        if self.train_finetune:\n",
    "            fmt+= f\"     Model filename: '{self.finetune_filename}'\\n\"\n",
    "        if self.finetune_resume_filename is not None:\n",
    "            fmt+= f\"     Resume filename: '{self.finetune_resume_filename}'\\n\"\n",
    "        if self.train_finetune and not self.only_print_reports:\n",
    "            fmt+= f\"     Maximum training epochs: {self.parse_value('finetune_max_epochs')}\\n\"\n",
    "            # batch size\n",
    "            fmt+= f\"     Batch size: {self.parse_value('finetune_batch_size')}\\n\"\n",
    "            # lr\n",
    "            fmt+= f\"     Learning Rate: {self.parse_value('finetune_lr')}\\n\"\n",
    "            # gamma\n",
    "            fmt+= f\"     Gamma: {self.parse_value('finetune_gamma')}\\n\"\n",
    "            db = self.get_db_name(model='finetune', db_name='db_train')\n",
    "            if db[0] is not None:\n",
    "                fmt+= f\"     Dataset: {db[0]}\"\n",
    "                if db[1] is not None:\n",
    "                    fmt+= f\" ('{db[1]}')\"\n",
    "                fmt+= '\\n'\n",
    "            fmt+= f\"     The dataset will {'' if self.parse_value('finetune_balance_dataset') else 'NOT '}be balanced.\\n\"\n",
    "            fmt+= f\"     The training dataset will {'' if self.label_finetune_dataset else 'NOT '}be labeled by the Oracle Model.\\n\"\n",
    "        else:\n",
    "            fmt+= f\"     It will NOT be trained.\\n\"\n",
    "        db_root = self.parse_value('finetune_dataset_root')\n",
    "        if db_root != '': fmt+= f\"     Dataset root: '{db_root}'\\n\"\n",
    "        return fmt\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        fmt = \"Options:\\n\"\n",
    "        fmt+= f\"  Problem: {self.problem_name}\\n\"\n",
    "        ## ORACLE\n",
    "        fmt+=self.__get_repr_oracle()\n",
    "        ## COPYCAT:\n",
    "        fmt+=self.__get_repr_copycat()\n",
    "        ## COPYCAT FINETUNE:\n",
    "        fmt+=self.__get_repr_finetune()\n",
    "        ## REPORTS:\n",
    "        fmt+='\\n'\n",
    "        if self.validation_step != 0 and not self.only_print_reports:\n",
    "            fmt+= f\"  Validation Steps: {self.parse_value('validation_step')}\\n\"\n",
    "            fmt+= f\"  A snapshot of the model will {'' if self.parse_value('save_snapshot') else 'NOT '}be saved for each validation step.\\n\"\n",
    "        \n",
    "        if self.only_print_reports:\n",
    "            fmt+= \"\\nNOTE: As 'only-print-reports' was selected, the models will only be loaded (or created with random parameters) and tested.\\n\"\n",
    "            fmt+= \"NOTE: THE MODELS WILL NOT BE TRAINED!!!\"\n",
    "        \n",
    "        fmt+= f\"\\nDevice to use: '{cuda.get_device_name()}'\\n\"\n",
    "        \n",
    "        if self.seed is not None:\n",
    "            fmt+= f\"\\nThe following seed will be used for Torch, Numpy and Random: {self.seed}\\n\"\n",
    "        return fmt\n",
    "\n",
    "def parse_boolean(value):\n",
    "    if isinstance(value, bool):\n",
    "        return value\n",
    "    if value.lower() in {'false', 'f', '0', 'no', 'n', 'not', 'dont'}:\n",
    "        return False\n",
    "    elif value.lower() in {'true', 't', '1', 'yes', 'y', 'yeah', 'yeap', 'ofcouse'}:\n",
    "        return True\n",
    "    raise ValueError(f'{value} is not a valid boolean value')\n",
    "\n",
    "def get_problem_names():\n",
    "    p_aux = argparse.ArgumentParser()\n",
    "    p_aux.add_argument('--config-file')\n",
    "    aux_arg, _ = p_aux.parse_known_args()    \n",
    "    return Config(aux_arg.config_file).get_problem_names()\n",
    "\n",
    "def parse_params():\n",
    "    problem_names = get_problem_names()\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-p', '--problem',  required=True, type=str, help='Problem name', choices=problem_names)\n",
    "\n",
    "    parser.add_argument('--config-file', help=\"Use this option to use a different configuration file and override the default options set in 'config.toml'\")\n",
    "    parser.add_argument('--only-print-reports', action='store_true', help='Use this option to only load the models and print their reports.')\n",
    "    parser.add_argument('--seed', type=int, help='Use this option to provide a new seed to Pytorch/Numpy/Random')\n",
    "\n",
    "    parser.add_argument('--validation-step', type=int, help=\"Change validation step. Set 0 (zero) to disable validation during training.\")\n",
    "    parser.add_argument('--save-snapshot', type=parse_boolean, nargs='?', const=True, help=\"Save snapshots at each validation step\")\n",
    "    #Oracle\n",
    "    parser.add_argument('--oracle', type=str, default='Oracle.pth', help=\"Filename to save the Oracle Model\")\n",
    "    parser.add_argument('--oracle-resume', type=str, help=\"Filename to resume the Oracle Model\")\n",
    "    parser.add_argument('--oracle-arch', type=str, default='vgg16', help=\"Oracle architecture (default: vgg16)\")\n",
    "    parser.add_argument('--dont-train-oracle', action='store_true', help=\"You can use this option to: Resume the Oracle's Model, or test the problem on an Oracle's Model with random weights\")\n",
    "    parser.add_argument('--oracle-max-epochs', type=int, help=\"Change maximum epochs to train Oracle's Model\")\n",
    "    parser.add_argument('--oracle-batch-size', type=int, help=\"Batch size to train Oracle's Model\")\n",
    "    parser.add_argument('--oracle-lr', type=float, help=\"Learning rate to train Oracle's Model\")\n",
    "    parser.add_argument('--oracle-gamma', type=float, help=\"Gamma to train Oracle's Model. It is the value to decrease the learning rate (lr*gamma)\")\n",
    "    parser.add_argument('--oracle-dataset-root', type=str, default='', help=\"Root folder of dataset files (image list and images listes in it)\")\n",
    "    #Copycat:\n",
    "    parser.add_argument('--copycat', type=str, default='Copycat.pth', help=\"Filename to save the Copycat Model\")\n",
    "    parser.add_argument('--copycat-resume', type=str, help=\"Filename to resume the Copycat Model\")\n",
    "    parser.add_argument('--copycat-arch', type=str, default='vgg16', help=\"Copycat (and Finetune) architecture (default: vgg16)\")\n",
    "    parser.add_argument('--dont-train-copycat', action='store_true', help=\"You can use this option to test a Copycat's Model with random weights\")\n",
    "    parser.add_argument('--copycat-max-epochs', type=int, help=\"Change maximum epochs to train Copycat's Model\")\n",
    "    parser.add_argument('--copycat-batch-size', type=int, help=\"Batch size to train Copycat's Model\")\n",
    "    parser.add_argument('--copycat-lr', type=float, help=\"Learning rate to train Copycat's Model\")\n",
    "    parser.add_argument('--copycat-gamma', type=float, help=\"Gamma to train Copycat's Model. It is the value to decrease the learning rate (lr*gamma)\")\n",
    "    parser.add_argument('--copycat-dataset-root', type=str, default='', help=\"Root folder of Copycat problem dataset\")\n",
    "    parser.add_argument('--copycat-balance-dataset', type=parse_boolean, help=\"Replicate or drop images to balance the number of images per class\")\n",
    "    parser.add_argument('--dont-label-copycat-dataset', action='store_true', help='Use this option to avoid labeling the Copycat training dataset (NPD)')\n",
    "    #Finetune:\n",
    "    parser.add_argument('--finetune', type=str, default='Copycat-Finetune.pth', help=\"Filename to save the Copycat Finetune Model\")\n",
    "    parser.add_argument('--finetune-resume', type=str, help=\"Filename to resume the Copycat Finetune Model\")\n",
    "    parser.add_argument('--dont-train-finetune', action='store_true', help=\"You can use this option to avoid finetuning Copycat model\")\n",
    "    parser.add_argument('--finetune-max-epochs', type=int, help=\"Change maximum epochs to finetune Copycat's Model\")\n",
    "    parser.add_argument('--finetune-batch-size', type=int, help=\"Batch size to finetune Copycat's Model\")\n",
    "    parser.add_argument('--finetune-lr', type=float, help=\"Learning rate to finetune Copycat's Model\")\n",
    "    parser.add_argument('--finetune-gamma', type=float, help=\"Gamma to finetune Copycat's Model. It is the value to decrease the learning rate (lr*gamma)\")\n",
    "    parser.add_argument('--finetune-dataset-root', type=str, default='', help=\"Root folder of Copycat problem dataset\")\n",
    "    parser.add_argument('--finetune-balance-dataset', type=parse_boolean, help=\"Replicate or drop images to balance the number of images per class\")\n",
    "    parser.add_argument('--dont-label-finetune-dataset', action='store_true', help='Use this option to avoid labeling the Copycat finetune dataset (NPD)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return Options(problem_name=args.problem,\n",
    "                   only_print_reports=args.only_print_reports,\n",
    "                   config_file=args.config_file, validation_step=args.validation_step, save_snapshot=args.save_snapshot,\n",
    "                   #oracle\n",
    "                   oracle_filename=args.oracle,\n",
    "                   oracle_resume_filename=args.oracle_resume,\n",
    "                   oracle_arch=args.oracle_arch,\n",
    "                   train_oracle=not args.dont_train_oracle,\n",
    "                   oracle_max_epochs=args.oracle_max_epochs,\n",
    "                   oracle_batch_size=args.oracle_batch_size,\n",
    "                   oracle_lr=args.oracle_lr,\n",
    "                   oracle_gamma=args.oracle_gamma,\n",
    "                   oracle_dataset_root=args.oracle_dataset_root,\n",
    "                   #copycat\n",
    "                   copycat_filename=args.copycat,\n",
    "                   copycat_resume_filename=args.copycat_resume,\n",
    "                   copycat_arch=args.copycat_arch,\n",
    "                   train_copycat=not args.dont_train_copycat,\n",
    "                   copycat_max_epochs=args.copycat_max_epochs,\n",
    "                   copycat_batch_size=args.copycat_batch_size,\n",
    "                   copycat_lr=args.copycat_lr,\n",
    "                   copycat_gamma=args.copycat_gamma,\n",
    "                   copycat_dataset_root=args.copycat_dataset_root,\n",
    "                   copycat_balance_dataset=args.copycat_balance_dataset,\n",
    "                   label_copycat_dataset=not args.dont_label_copycat_dataset,\n",
    "                   #finetune\n",
    "                   finetune_filename=args.finetune,\n",
    "                   finetune_resume_filename=args.finetune_resume,\n",
    "                   train_finetune=not args.dont_train_finetune,\n",
    "                   finetune_max_epochs=args.finetune_max_epochs,\n",
    "                   finetune_batch_size=args.finetune_batch_size,\n",
    "                   finetune_lr=args.finetune_lr,\n",
    "                   finetune_gamma=args.finetune_gamma,\n",
    "                   finetune_dataset_root=args.finetune_dataset_root,\n",
    "                   finetune_balance_dataset=args.finetune_balance_dataset,\n",
    "                   label_finetune_dataset=not args.dont_label_finetune_dataset,\n",
    "                   #seed\n",
    "                   seed=args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae0741-906f-4762-99a0-1e20da20ee42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c48bb1-6ea1-4900-83b7-3b81e6d5b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_epochs = 10\n",
    "copycat_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37e3b680-d4c4-4883-a0a5-ddafde5e1a66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options:\n",
      "  Problem: MNIST\n",
      "  Oracle:\n",
      "     Model filename: 'mnist.pth'\n",
      "     Model arch: resnet18\n",
      "     Maximum training epochs: 10\n",
      "     Batch size: 32\n",
      "     Learning Rate: 0.0001\n",
      "     Gamma: 0.3\n",
      "     Dataset: od1 ('/home/jeiks/Christian/data/mnist_train.txt')\n",
      "     Test dataset: test1 ('/home/jeiks/Christian/data/mnist_test.txt')\n",
      "  Copycat:\n",
      "     Model filename: 'copycat_mnist.pth'\n",
      "     Model arch: resnet18\n",
      "     Maximum training epochs: 5\n",
      "     Batch size: 32\n",
      "     Learning Rate: 0.0001\n",
      "     Gamma: 0.3\n",
      "     Dataset: npd_5k1 ('/home/jeiks/Christian/data/npd.txt')\n",
      "     Test dataset: test1 ('/home/jeiks/Christian/data/mnist_test.txt')\n",
      "     The training dataset will be balanced.\n",
      "     The training dataset will be labeled by the Oracle Model.\n",
      "  Copycat Finetuning:\n",
      "     It will NOT be trained.\n",
      "\n",
      "  Validation Steps: 2\n",
      "  A snapshot of the model will be saved for each validation step.\n",
      "\n",
      "Device to use: 'NVIDIA GeForce RTX 4070'\n",
      "\n",
      "==> Training Oracle (Mon Feb 17 22:19:40 2025):\n",
      "(Oracle) Starting a new model with random parameters...\n",
      "Scheduler Milestones: [5, 8, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 01/10 Loss: 0.076470 Accuracy: 93.72% lr: 1.00e-04 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:22<00:00, 82.10it/s]\n",
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:01<00:00, 217.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ F1 Score on test1 dataset:\n",
      "~~ Micro Avg: 0.985400 Macro Avg: 0.985289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 02/10 Loss: 0.047293 Accuracy: 98.33% lr: 1.00e-04 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:22<00:00, 82.36it/s]\n",
      "[ Epoch: 03/10 Loss: 0.040451 Accuracy: 98.75% lr: 1.00e-04 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:22<00:00, 82.93it/s]\n",
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:01<00:00, 228.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ F1 Score on test1 dataset:\n",
      "~~ Micro Avg: 0.991000 Macro Avg: 0.990940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 04/10 Loss: 0.035500 Accuracy: 99.09% lr: 1.00e-04 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:22<00:00, 82.87it/s]\n",
      "[ Epoch: 05/10 Loss: 0.025433 Accuracy: 99.18% lr: 1.00e-04 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:22<00:00, 82.66it/s]\n",
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:01<00:00, 226.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ F1 Score on test1 dataset:\n",
      "~~ Micro Avg: 0.992900 Macro Avg: 0.992849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 06/10 Loss: 0.021101 Accuracy: 99.31% lr: 3.00e-05 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:22<00:00, 82.76it/s]\n",
      "[ Epoch: 07/10 Loss: 0.026347 Accuracy: 99.41% lr: 3.00e-05 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:22<00:00, 82.74it/s]\n",
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:01<00:00, 224.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ F1 Score on test1 dataset:\n",
      "~~ Micro Avg: 0.993000 Macro Avg: 0.992952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 08/10 Loss: 0.012179 Accuracy: 99.40% lr: 3.00e-05 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:22<00:00, 82.52it/s]\n",
      "[ Epoch: 09/10 Loss: 0.020928 Accuracy: 99.42% lr: 9.00e-06 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:22<00:00, 82.56it/s]\n",
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:01<00:00, 225.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ F1 Score on test1 dataset:\n",
      "~~ Micro Avg: 0.993600 Macro Avg: 0.993558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 10/10 Loss: 0.015272 Accuracy: 99.36% lr: 2.70e-06 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:22<00:00, 82.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model in: \"mnist.pth\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:01<00:00, 225.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle reports:\n",
      "Training - Dataset ImageList:\n",
      "    Number of datapoints: 60000\n",
      "     * Class 0:   5923 samples\n",
      "     * Class 1:   6742 samples\n",
      "     * Class 2:   5958 samples\n",
      "     * Class 3:   6131 samples\n",
      "     * Class 4:   5842 samples\n",
      "     * Class 5:   5421 samples\n",
      "     * Class 6:   5918 samples\n",
      "     * Class 7:   6265 samples\n",
      "     * Class 8:   5851 samples\n",
      "     * Class 9:   5949 samples\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.1307, 0.1307, 0.1307), std=(0.2818, 0.2818, 0.2818))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "Testing - Dataset ImageList:\n",
      "    Number of datapoints: 10000\n",
      "     * Class 0:    980 samples\n",
      "     * Class 1:   1135 samples\n",
      "     * Class 2:   1032 samples\n",
      "     * Class 3:   1010 samples\n",
      "     * Class 4:    982 samples\n",
      "     * Class 5:    892 samples\n",
      "     * Class 6:    958 samples\n",
      "     * Class 7:   1028 samples\n",
      "     * Class 8:    974 samples\n",
      "     * Class 9:   1009 samples\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.1307, 0.1307, 0.1307), std=(0.2818, 0.2818, 0.2818))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "\n",
      "Metrics:\n",
      "Confusion Matrix:\n",
      "          0    1    2    3    4    5    6    7    8    9\n",
      "   0:   978    0    0    0    0    0    1    0    1    0\n",
      "   1:     0 1131    2    0    0    0    1    1    0    0\n",
      "   2:     1    0 1028    0    1    0    0    2    0    0\n",
      "   3:     0    0    0 1006    0    2    0    0    2    0\n",
      "   4:     0    1    0    0  977    0    0    1    1    2\n",
      "   5:     1    0    0    4    0  883    1    1    0    2\n",
      "   6:     6    4    0    0    2    1  945    0    0    0\n",
      "   7:     0    2    3    0    2    0    0 1020    1    0\n",
      "   8:     0    0    0    2    0    1    0    0  969    2\n",
      "   9:     0    0    0    2    5    1    0    2    0  999\n",
      "--------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.991886  0.997959  0.994914       980\n",
      "           1   0.993849  0.996476  0.995161      1135\n",
      "           2   0.995160  0.996124  0.995642      1032\n",
      "           3   0.992110  0.996040  0.994071      1010\n",
      "           4   0.989868  0.994908  0.992382       982\n",
      "           5   0.994369  0.989910  0.992135       892\n",
      "           6   0.996835  0.986430  0.991605       958\n",
      "           7   0.993184  0.992218  0.992701      1028\n",
      "           8   0.994867  0.994867  0.994867       974\n",
      "           9   0.994030  0.990089  0.992056      1009\n",
      "\n",
      "    accuracy                       0.993600     10000\n",
      "   macro avg   0.993616  0.993502  0.993553     10000\n",
      "weighted avg   0.993607  0.993600  0.993598     10000\n",
      "-----------------------------------------------------\n",
      "Accuracy    Macro Average: 0.993502\n",
      "Accuracy F1-Macro Average: 0.993553\n",
      "\n",
      "\n",
      "==> Training Copycat (Mon Feb 17 22:23:40 2025):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeiks/Christian/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jeiks/Christian/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Copycat) Starting a new model with random parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Labeling dataset \"npd_5k1\" ]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40037/40037 [17:30<00:00, 38.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduler Milestones: [2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 01/5 Loss: 0.741111 Accuracy: 62.90% lr: 1.00e-04 ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 40037/40037 [08:09<00:00, 81.85it/s]\n",
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:01<00:00, 211.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ F1 Score on test1 dataset:\n",
      "~~ Micro Avg: 0.625100 Macro Avg: 0.573053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 02/5 Loss: 0.510248 Accuracy: 77.64% lr: 1.00e-04 ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 40037/40037 [08:09<00:00, 81.75it/s]\n",
      "[ Epoch: 03/5 Loss: 0.509123 Accuracy: 82.66% lr: 3.00e-05 ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 40037/40037 [08:11<00:00, 81.42it/s]\n",
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:01<00:00, 210.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ F1 Score on test1 dataset:\n",
      "~~ Micro Avg: 0.772900 Macro Avg: 0.761489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 04/5 Loss: 0.426576 Accuracy: 84.06% lr: 9.00e-06 ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 40037/40037 [08:10<00:00, 81.61it/s]\n",
      "[ Epoch: 05/5 Loss: 0.441778 Accuracy: 84.48% lr: 9.00e-06 ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 40037/40037 [08:11<00:00, 81.53it/s]\n",
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:01<00:00, 210.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ F1 Score on test1 dataset:\n",
      "~~ Micro Avg: 0.801600 Macro Avg: 0.790947\n",
      "Saving Copycat model in: \"copycat_mnist.pth\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:01<00:00, 209.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copycat reports:\n",
      "Training - Dataset ImageList (Balanced):\n",
      "    Number of datapoints: 1281160\n",
      "     * Samples per class: 128116\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "                             ToTensor()\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "Testing - Dataset ImageList:\n",
      "    Number of datapoints: 10000\n",
      "     * Class 0:    980 samples\n",
      "     * Class 1:   1135 samples\n",
      "     * Class 2:   1032 samples\n",
      "     * Class 3:   1010 samples\n",
      "     * Class 4:    982 samples\n",
      "     * Class 5:    892 samples\n",
      "     * Class 6:    958 samples\n",
      "     * Class 7:   1028 samples\n",
      "     * Class 8:    974 samples\n",
      "     * Class 9:   1009 samples\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "                             ToTensor()\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "\n",
      "Metrics:\n",
      "Confusion Matrix:\n",
      "          0    1    2    3    4    5    6    7    8    9\n",
      "   0:   974    1    0    0    0    2    0    0    0    3\n",
      "   1:     1 1123    1    2    1    4    0    3    0    0\n",
      "   2:     7   26  693   13    6  178    1   79    0   29\n",
      "   3:     5    0   81  710    0  115    0   43    1   55\n",
      "   4:     0   23    1    1  883    1    0   42    3   28\n",
      "   5:    16    0   14    5    0  842    1    6    1    7\n",
      "   6:   275   23  149    1    3    9  488    3    0    7\n",
      "   7:     1   14    2    0    3   96    0  886    0   26\n",
      "   8:    11    4  157   21   28   45    0   14  443  251\n",
      "   9:     9    2    5    3    4   11    0    1    0  974\n",
      "--------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.749808  0.993878  0.854761       980\n",
      "           1   0.923520  0.989427  0.955338      1135\n",
      "           2   0.628286  0.671512  0.649180      1032\n",
      "           3   0.939153  0.702970  0.804077      1010\n",
      "           4   0.951509  0.899185  0.924607       982\n",
      "           5   0.646201  0.943946  0.767198       892\n",
      "           6   0.995918  0.509395  0.674033       958\n",
      "           7   0.822656  0.861868  0.841805      1028\n",
      "           8   0.988839  0.454825  0.623066       974\n",
      "           9   0.705797  0.965312  0.815404      1009\n",
      "\n",
      "    accuracy                       0.801600     10000\n",
      "   macro avg   0.835169  0.799232  0.790947     10000\n",
      "weighted avg   0.836579  0.801600  0.793706     10000\n",
      "-----------------------------------------------------\n",
      "Accuracy    Macro Average: 0.799232\n",
      "Accuracy F1-Macro Average: 0.790947\n",
      "\n",
      "\n",
      "Done (Mon Feb 17 23:22:15 2025)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeiks/Christian/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jeiks/Christian/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Finetune) Loading model from \"copycat_mnist.pth\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:01<00:00, 203.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Macro F1-Score):\n",
      "  Oracle...: 0.993553\n",
      "  Copycat..: 0.790947\n",
      "  Finetune.: 0.790947\n",
      "Attack Performance\n",
      "  Copycat on Oracle...........: 79.61%\n",
      "  Finetuned Copycat on Oracle.: 79.61%\n",
      "Finished (Mon Feb 17 23:22:22 2025)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oracle_epochs = 10\n",
    "copycat_epochs = 5\n",
    "seed = set_seeds(7)\n",
    "if __name__ == \"__main__\":\n",
    "    options_mnist = Options(problem_name='MNIST',\n",
    "                   only_print_reports=None,\n",
    "                   config_file=None, validation_step=2, save_snapshot=1,\n",
    "                   #oracle\n",
    "                   oracle_filename='mnist.pth',\n",
    "                   oracle_resume_filename=None,\n",
    "                   oracle_arch='resnet18',\n",
    "                   train_oracle=True,\n",
    "                   oracle_max_epochs=oracle_epochs,\n",
    "                   oracle_batch_size=32,\n",
    "                   oracle_lr=0.0001,\n",
    "                   oracle_gamma=0.3,\n",
    "                   oracle_dataset_root='',\n",
    "                   #copycat\n",
    "                   copycat_filename='copycat_mnist.pth',\n",
    "                   copycat_resume_filename=None,\n",
    "                   copycat_arch='resnet18',\n",
    "                   train_copycat=True,\n",
    "                   copycat_max_epochs=copycat_epochs,\n",
    "                   copycat_batch_size=32,\n",
    "                   copycat_lr=0.0001,\n",
    "                   copycat_gamma=0.3,\n",
    "                   copycat_dataset_root='',\n",
    "                   copycat_balance_dataset=None,\n",
    "                   label_copycat_dataset=True,\n",
    "                   #finetune\n",
    "                   finetune_filename='',\n",
    "                   finetune_resume_filename=None,\n",
    "                   train_finetune=False,\n",
    "                   finetune_max_epochs=None,\n",
    "                   finetune_batch_size=None,\n",
    "                   finetune_lr=None,\n",
    "                   finetune_gamma=None,\n",
    "                   finetune_dataset_root='',\n",
    "                   finetune_balance_dataset=None,\n",
    "                   label_finetune_dataset=False,\n",
    "                   #seed\n",
    "                   seed=seed)\n",
    "    print(options_mnist)\n",
    "    #input('\\nCheck the parameters and press ENTER to continue...\\n')\n",
    "    problem = Problem(problem=options_mnist.problem_name,\n",
    "                      #oracle\n",
    "                      oracle_arch=options_mnist.oracle_arch,\n",
    "                      oracle_filename=options_mnist.oracle_filename,\n",
    "                      oracle_resume_filename=options_mnist.oracle_resume_filename,\n",
    "                      oracle_dataset_root=options_mnist.oracle_dataset_root,\n",
    "                      #copycat\n",
    "                      copycat_arch=options_mnist.copycat_arch,\n",
    "                      copycat_filename=options_mnist.copycat_filename,\n",
    "                      copycat_resume_filename=options_mnist.copycat_resume_filename,\n",
    "                      copycat_dataset_root=options_mnist.copycat_dataset_root,\n",
    "                      #finetune\n",
    "                      finetune_filename=options_mnist.finetune_filename,\n",
    "                      finetune_resume_filename=options_mnist.finetune_resume_filename,\n",
    "                      finetune_dataset_root=options_mnist.finetune_dataset_root,\n",
    "                      #configuration file\n",
    "                      config_fn=options_mnist.config_file,\n",
    "                      #seed\n",
    "                      seed=options_mnist.seed)\n",
    "\n",
    "    problem.run(validation_step=options_mnist.validation_step,\n",
    "                    save_snapshot=options_mnist.save_snapshot,\n",
    "                    #Oracle options:\n",
    "                    train_oracle=options_mnist.train_oracle,\n",
    "                    oracle_max_epochs=options_mnist.oracle_max_epochs,\n",
    "                    oracle_batch_size=options_mnist.oracle_batch_size,\n",
    "                    oracle_lr=options_mnist.oracle_lr,\n",
    "                    oracle_gamma=options_mnist.oracle_gamma,\n",
    "                    #Copycat options:\n",
    "                    train_copycat=options_mnist.train_copycat,\n",
    "                    label_copycat_dataset=options_mnist.label_copycat_dataset,\n",
    "                    copycat_max_epochs=options_mnist.copycat_max_epochs,\n",
    "                    copycat_batch_size=options_mnist.copycat_batch_size,\n",
    "                    copycat_lr=options_mnist.copycat_lr,\n",
    "                    copycat_gamma=options_mnist.copycat_gamma,\n",
    "                    copycat_balance_dataset=options_mnist.copycat_balance_dataset,\n",
    "                    #Copycat Finetune options:\n",
    "                    finetune_copycat=options_mnist.train_finetune,\n",
    "                    label_finetune_dataset=options_mnist.label_finetune_dataset,\n",
    "                    finetune_max_epochs=options_mnist.finetune_max_epochs,\n",
    "                    finetune_batch_size=options_mnist.finetune_batch_size,\n",
    "                    finetune_lr=options_mnist.finetune_lr,\n",
    "                    finetune_gamma=options_mnist.finetune_gamma,\n",
    "                    finetune_balance_dataset=options_mnist.finetune_balance_dataset\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "579f914b-8966-4b3b-a449-94ba2d9af4cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:01<00:00, 202.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle reports:\n",
      "Training - Dataset ImageList:\n",
      "    Number of datapoints: 60000\n",
      "     * Class 0:   5923 samples\n",
      "     * Class 1:   6742 samples\n",
      "     * Class 2:   5958 samples\n",
      "     * Class 3:   6131 samples\n",
      "     * Class 4:   5842 samples\n",
      "     * Class 5:   5421 samples\n",
      "     * Class 6:   5918 samples\n",
      "     * Class 7:   6265 samples\n",
      "     * Class 8:   5851 samples\n",
      "     * Class 9:   5949 samples\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.1307, 0.1307, 0.1307), std=(0.2818, 0.2818, 0.2818))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "Testing - Dataset ImageList:\n",
      "    Number of datapoints: 10000\n",
      "     * Class 0:    980 samples\n",
      "     * Class 1:   1135 samples\n",
      "     * Class 2:   1032 samples\n",
      "     * Class 3:   1010 samples\n",
      "     * Class 4:    982 samples\n",
      "     * Class 5:    892 samples\n",
      "     * Class 6:    958 samples\n",
      "     * Class 7:   1028 samples\n",
      "     * Class 8:    974 samples\n",
      "     * Class 9:   1009 samples\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.1307, 0.1307, 0.1307), std=(0.2818, 0.2818, 0.2818))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "\n",
      "Metrics:\n",
      "Confusion Matrix:\n",
      "          0    1    2    3    4    5    6    7    8    9\n",
      "   0:   978    0    0    0    0    0    1    0    1    0\n",
      "   1:     0 1131    2    0    0    0    1    1    0    0\n",
      "   2:     1    0 1028    0    1    0    0    2    0    0\n",
      "   3:     0    0    0 1006    0    2    0    0    2    0\n",
      "   4:     0    1    0    0  977    0    0    1    1    2\n",
      "   5:     1    0    0    4    0  883    1    1    0    2\n",
      "   6:     6    4    0    0    2    1  945    0    0    0\n",
      "   7:     0    2    3    0    2    0    0 1020    1    0\n",
      "   8:     0    0    0    2    0    1    0    0  969    2\n",
      "   9:     0    0    0    2    5    1    0    2    0  999\n",
      "--------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.991886  0.997959  0.994914       980\n",
      "           1   0.993849  0.996476  0.995161      1135\n",
      "           2   0.995160  0.996124  0.995642      1032\n",
      "           3   0.992110  0.996040  0.994071      1010\n",
      "           4   0.989868  0.994908  0.992382       982\n",
      "           5   0.994369  0.989910  0.992135       892\n",
      "           6   0.996835  0.986430  0.991605       958\n",
      "           7   0.993184  0.992218  0.992701      1028\n",
      "           8   0.994867  0.994867  0.994867       974\n",
      "           9   0.994030  0.990089  0.992056      1009\n",
      "\n",
      "    accuracy                       0.993600     10000\n",
      "   macro avg   0.993616  0.993502  0.993553     10000\n",
      "weighted avg   0.993607  0.993600  0.993598     10000\n",
      "-----------------------------------------------------\n",
      "Accuracy    Macro Average: 0.993502\n",
      "Accuracy F1-Macro Average: 0.993553\n",
      "\n",
      "\n",
      "Copycat:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:01<00:00, 209.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copycat reports:\n",
      "Training - Dataset ImageList (Balanced):\n",
      "    Number of datapoints: 1281160\n",
      "     * Samples per class: 128116\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "                             ToTensor()\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "Testing - Dataset ImageList:\n",
      "    Number of datapoints: 10000\n",
      "     * Class 0:    980 samples\n",
      "     * Class 1:   1135 samples\n",
      "     * Class 2:   1032 samples\n",
      "     * Class 3:   1010 samples\n",
      "     * Class 4:    982 samples\n",
      "     * Class 5:    892 samples\n",
      "     * Class 6:    958 samples\n",
      "     * Class 7:   1028 samples\n",
      "     * Class 8:    974 samples\n",
      "     * Class 9:   1009 samples\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "                             ToTensor()\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "\n",
      "Metrics:\n",
      "Confusion Matrix:\n",
      "          0    1    2    3    4    5    6    7    8    9\n",
      "   0:   974    1    0    0    0    2    0    0    0    3\n",
      "   1:     1 1123    1    2    1    4    0    3    0    0\n",
      "   2:     7   26  693   13    6  178    1   79    0   29\n",
      "   3:     5    0   81  710    0  115    0   43    1   55\n",
      "   4:     0   23    1    1  883    1    0   42    3   28\n",
      "   5:    16    0   14    5    0  842    1    6    1    7\n",
      "   6:   275   23  149    1    3    9  488    3    0    7\n",
      "   7:     1   14    2    0    3   96    0  886    0   26\n",
      "   8:    11    4  157   21   28   45    0   14  443  251\n",
      "   9:     9    2    5    3    4   11    0    1    0  974\n",
      "--------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.749808  0.993878  0.854761       980\n",
      "           1   0.923520  0.989427  0.955338      1135\n",
      "           2   0.628286  0.671512  0.649180      1032\n",
      "           3   0.939153  0.702970  0.804077      1010\n",
      "           4   0.951509  0.899185  0.924607       982\n",
      "           5   0.646201  0.943946  0.767198       892\n",
      "           6   0.995918  0.509395  0.674033       958\n",
      "           7   0.822656  0.861868  0.841805      1028\n",
      "           8   0.988839  0.454825  0.623066       974\n",
      "           9   0.705797  0.965312  0.815404      1009\n",
      "\n",
      "    accuracy                       0.801600     10000\n",
      "   macro avg   0.835169  0.799232  0.790947     10000\n",
      "weighted avg   0.836579  0.801600  0.793706     10000\n",
      "-----------------------------------------------------\n",
      "Accuracy    Macro Average: 0.799232\n",
      "Accuracy F1-Macro Average: 0.790947\n",
      "\n",
      "\n",
      "Finetuned Copycat:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:01<00:00, 210.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copycat reports:\n",
      "Training - Dataset ImageList:\n",
      "    Number of datapoints: 10000\n",
      "     * Class 0:    980 samples\n",
      "     * Class 1:   1135 samples\n",
      "     * Class 2:   1032 samples\n",
      "     * Class 3:   1010 samples\n",
      "     * Class 4:    982 samples\n",
      "     * Class 5:    892 samples\n",
      "     * Class 6:    958 samples\n",
      "     * Class 7:   1028 samples\n",
      "     * Class 8:    974 samples\n",
      "     * Class 9:   1009 samples\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "                             ToTensor()\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "Testing - Dataset ImageList:\n",
      "    Number of datapoints: 10000\n",
      "     * Class 0:    980 samples\n",
      "     * Class 1:   1135 samples\n",
      "     * Class 2:   1032 samples\n",
      "     * Class 3:   1010 samples\n",
      "     * Class 4:    982 samples\n",
      "     * Class 5:    892 samples\n",
      "     * Class 6:    958 samples\n",
      "     * Class 7:   1028 samples\n",
      "     * Class 8:    974 samples\n",
      "     * Class 9:   1009 samples\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "                             ToTensor()\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "\n",
      "Metrics:\n",
      "Confusion Matrix:\n",
      "          0    1    2    3    4    5    6    7    8    9\n",
      "   0:   974    1    0    0    0    2    0    0    0    3\n",
      "   1:     1 1123    1    2    1    4    0    3    0    0\n",
      "   2:     7   26  693   13    6  178    1   79    0   29\n",
      "   3:     5    0   81  710    0  115    0   43    1   55\n",
      "   4:     0   23    1    1  883    1    0   42    3   28\n",
      "   5:    16    0   14    5    0  842    1    6    1    7\n",
      "   6:   275   23  149    1    3    9  488    3    0    7\n",
      "   7:     1   14    2    0    3   96    0  886    0   26\n",
      "   8:    11    4  157   21   28   45    0   14  443  251\n",
      "   9:     9    2    5    3    4   11    0    1    0  974\n",
      "--------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.749808  0.993878  0.854761       980\n",
      "           1   0.923520  0.989427  0.955338      1135\n",
      "           2   0.628286  0.671512  0.649180      1032\n",
      "           3   0.939153  0.702970  0.804077      1010\n",
      "           4   0.951509  0.899185  0.924607       982\n",
      "           5   0.646201  0.943946  0.767198       892\n",
      "           6   0.995918  0.509395  0.674033       958\n",
      "           7   0.822656  0.861868  0.841805      1028\n",
      "           8   0.988839  0.454825  0.623066       974\n",
      "           9   0.705797  0.965312  0.815404      1009\n",
      "\n",
      "    accuracy                       0.801600     10000\n",
      "   macro avg   0.835169  0.799232  0.790947     10000\n",
      "weighted avg   0.836579  0.801600  0.793706     10000\n",
      "-----------------------------------------------------\n",
      "Accuracy    Macro Average: 0.799232\n",
      "Accuracy F1-Macro Average: 0.790947\n",
      "\n",
      "\n",
      "Accuracy (Macro F1-Score):\n",
      "  Oracle...: 0.993553\n",
      "  Copycat..: 0.790947\n",
      "  Finetune.: 0.790947\n",
      "Attack Performance\n",
      "  Copycat on Oracle...........: 79.61%\n",
      "  Finetuned Copycat on Oracle.: 79.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "problem.print_reports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbbef0e7-19f6-4931-a188-8d2a32462b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "teste a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training Oracle (Tue Feb 18 17:49:49 2025):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeiks/Christian/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jeiks/Christian/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Oracle) Starting a new model with random parameters...\n",
      "Scheduler Milestones: [5, 8, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 01/10 Loss: 0.605778 Accuracy: 60.75% lr: 1.00e-04 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2290/2290 [00:28<00:00, 81.26it/s]\n",
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 814/814 [00:03<00:00, 208.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ F1 Score on test dataset:\n",
      "~~ Micro Avg: 0.860057 Macro Avg: 0.847766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 02/10 Loss: 0.415703 Accuracy: 86.85% lr: 1.00e-04 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2290/2290 [00:28<00:00, 81.66it/s]\n",
      "[ Epoch: 03/10 Loss: 0.361758 Accuracy: 90.10% lr: 1.00e-04 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2290/2290 [00:27<00:00, 81.82it/s]\n",
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 814/814 [00:03<00:00, 212.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ F1 Score on test dataset:\n",
      "~~ Micro Avg: 0.924862 Macro Avg: 0.918063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 04/10 Loss: 0.274034 Accuracy: 91.78% lr: 1.00e-04 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2290/2290 [00:27<00:00, 81.82it/s]\n",
      "[ Epoch: 05/10 Loss: 0.205755 Accuracy: 92.83% lr: 1.00e-04 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2290/2290 [00:28<00:00, 81.68it/s]\n",
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 814/814 [00:03<00:00, 215.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ F1 Score on test dataset:\n",
      "~~ Micro Avg: 0.937346 Macro Avg: 0.931924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 06/10 Loss: 0.213594 Accuracy: 93.59% lr: 3.00e-05 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2290/2290 [00:28<00:00, 81.59it/s]\n",
      "[ Epoch: 07/10 Loss: 0.213063 Accuracy: 93.84% lr: 3.00e-05 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2290/2290 [00:28<00:00, 81.71it/s]\n",
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 814/814 [00:03<00:00, 211.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ F1 Score on test dataset:\n",
      "~~ Micro Avg: 0.940919 Macro Avg: 0.935359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 08/10 Loss: 0.238366 Accuracy: 93.95% lr: 3.00e-05 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2290/2290 [00:28<00:00, 81.71it/s]\n",
      "[ Epoch: 09/10 Loss: 0.164375 Accuracy: 94.15% lr: 9.00e-06 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2290/2290 [00:28<00:00, 81.57it/s]\n",
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 814/814 [00:03<00:00, 208.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ F1 Score on test dataset:\n",
      "~~ Micro Avg: 0.941341 Macro Avg: 0.935805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 10/10 Loss: 0.167478 Accuracy: 94.28% lr: 2.70e-06 ]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2290/2290 [00:28<00:00, 81.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model in: \"svhn.pth\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 814/814 [00:03<00:00, 204.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle reports:\n",
      "Training - Dataset ImageList:\n",
      "    Number of datapoints: 73257\n",
      "     * Class 0:   4948 samples\n",
      "     * Class 1:  13861 samples\n",
      "     * Class 2:  10585 samples\n",
      "     * Class 3:   8497 samples\n",
      "     * Class 4:   7458 samples\n",
      "     * Class 5:   6882 samples\n",
      "     * Class 6:   5727 samples\n",
      "     * Class 7:   5595 samples\n",
      "     * Class 8:   5045 samples\n",
      "     * Class 9:   4659 samples\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.4381, 0.4442, 0.4732), std=(0.117, 0.12, 0.1025))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "Testing - Dataset ImageList:\n",
      "    Number of datapoints: 26032\n",
      "     * Class 0:   1744 samples\n",
      "     * Class 1:   5099 samples\n",
      "     * Class 2:   4149 samples\n",
      "     * Class 3:   2882 samples\n",
      "     * Class 4:   2523 samples\n",
      "     * Class 5:   2384 samples\n",
      "     * Class 6:   1977 samples\n",
      "     * Class 7:   2019 samples\n",
      "     * Class 8:   1660 samples\n",
      "     * Class 9:   1595 samples\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.4381, 0.4442, 0.4732), std=(0.117, 0.12, 0.1025))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "\n",
      "Metrics:\n",
      "Confusion Matrix:\n",
      "          0    1    2    3    4    5    6    7    8    9\n",
      "   0:  1646   11    7    6    1    8   37    7    3   18\n",
      "   1:    17 4946   29   14   22    7    8   47    7    2\n",
      "   2:     5   29 3986   42   14   14    3   42    2   12\n",
      "   3:     4   44   26 2609    6   61    9    8   31   84\n",
      "   4:     8   55   11   12 2397    7    6    9    9    9\n",
      "   5:     5   19    5   70    7 2221   37    3    7   10\n",
      "   6:    19   15    2    9   11   24 1854    2   33    8\n",
      "   7:     3   57   23    5    2    2    2 1919    2    4\n",
      "   8:     8    8   11   34   10   16   60    3 1494   16\n",
      "   9:    43   10   35   15    5   11    6    3   23 1444\n",
      "--------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.936291  0.943807  0.940034      1744\n",
      "           1   0.952253  0.969994  0.961041      5099\n",
      "           2   0.963966  0.960713  0.962337      4149\n",
      "           3   0.926491  0.905274  0.915760      2882\n",
      "           4   0.968485  0.950059  0.959184      2523\n",
      "           5   0.936736  0.931628  0.934175      2384\n",
      "           6   0.916914  0.937785  0.927232      1977\n",
      "           7   0.939305  0.950471  0.944855      2019\n",
      "           8   0.927374  0.900000  0.913482      1660\n",
      "           9   0.898569  0.905329  0.901936      1595\n",
      "\n",
      "    accuracy                       0.941764     26032\n",
      "   macro avg   0.936638  0.935506  0.936004     26032\n",
      "weighted avg   0.941787  0.941764  0.941710     26032\n",
      "-----------------------------------------------------\n",
      "Accuracy    Macro Average: 0.935506\n",
      "Accuracy F1-Macro Average: 0.936004\n",
      "\n",
      "\n",
      "==> Training Copycat (Tue Feb 18 17:54:57 2025):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeiks/Christian/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jeiks/Christian/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Copycat) Loading model from \"weights_primeiro/copycat_mnist.pth\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Labeling dataset \"npd_5k\" ]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40037/40037 [16:41<00:00, 39.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduler Milestones: [2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 01/5 Loss: 0.924825 Accuracy: 57.84% lr: 1.00e-04 ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 40037/40037 [08:09<00:00, 81.76it/s]\n",
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 814/814 [00:03<00:00, 219.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ F1 Score on test dataset:\n",
      "~~ Micro Avg: 0.894246 Macro Avg: 0.890336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 02/5 Loss: 0.694760 Accuracy: 71.68% lr: 1.00e-04 ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 40037/40037 [08:09<00:00, 81.77it/s]\n",
      "[ Epoch: 03/5 Loss: 0.564716 Accuracy: 78.55% lr: 3.00e-05 ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 40037/40037 [08:12<00:00, 81.37it/s]\n",
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 814/814 [00:03<00:00, 219.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ F1 Score on test dataset:\n",
      "~~ Micro Avg: 0.924324 Macro Avg: 0.918190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Epoch: 04/5 Loss: 0.538885 Accuracy: 80.80% lr: 9.00e-06 ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 40037/40037 [08:11<00:00, 81.48it/s]\n",
      "[ Epoch: 05/5 Loss: 0.501517 Accuracy: 81.49% lr: 9.00e-06 ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 40037/40037 [08:10<00:00, 81.57it/s]\n",
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 814/814 [00:03<00:00, 219.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ F1 Score on test dataset:\n",
      "~~ Micro Avg: 0.924093 Macro Avg: 0.917961\n",
      "Saving Copycat model in: \"copycat_mnist_svhn.pth\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 814/814 [00:03<00:00, 220.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copycat reports:\n",
      "Training - Dataset ImageList (Balanced):\n",
      "    Number of datapoints: 1281160\n",
      "     * Samples per class: 128116\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "                             ToTensor()\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "Testing - Dataset ImageList:\n",
      "    Number of datapoints: 26032\n",
      "     * Class 0:   1744 samples\n",
      "     * Class 1:   5099 samples\n",
      "     * Class 2:   4149 samples\n",
      "     * Class 3:   2882 samples\n",
      "     * Class 4:   2523 samples\n",
      "     * Class 5:   2384 samples\n",
      "     * Class 6:   1977 samples\n",
      "     * Class 7:   2019 samples\n",
      "     * Class 8:   1660 samples\n",
      "     * Class 9:   1595 samples\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "                             ToTensor()\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "\n",
      "Metrics:\n",
      "Confusion Matrix:\n",
      "          0    1    2    3    4    5    6    7    8    9\n",
      "   0:  1663    5    5    5    1    8   27    6    3   21\n",
      "   1:    52 4712   50   31   32   20   11  180    6    5\n",
      "   2:     5   15 3940   42    9   17    6   89    8   18\n",
      "   3:     9   25   29 2513    9  146   10   24   28   89\n",
      "   4:    12   59   16   22 2329   25    9   25    6   20\n",
      "   5:     4   12    5   48    5 2258   31    4    7   10\n",
      "   6:    32   13    3    8    8   64 1809    5   29    6\n",
      "   7:     3   19   18    6    2    6    2 1961    0    2\n",
      "   8:    13    6   15   38    5   46   63    7 1442   25\n",
      "   9:    52    7   37   16    3   17    6   12   16 1429\n",
      "--------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.901355  0.953555  0.926721      1744\n",
      "           1   0.966961  0.924103  0.945046      5099\n",
      "           2   0.956775  0.949626  0.953187      4149\n",
      "           3   0.920850  0.871964  0.895741      2882\n",
      "           4   0.969205  0.923107  0.945595      2523\n",
      "           5   0.866130  0.947148  0.904829      2384\n",
      "           6   0.916413  0.915023  0.915718      1977\n",
      "           7   0.847817  0.971273  0.905355      2019\n",
      "           8   0.933333  0.868675  0.899844      1660\n",
      "           9   0.879385  0.895925  0.887578      1595\n",
      "\n",
      "    accuracy                       0.924093     26032\n",
      "   macro avg   0.915822  0.922040  0.917961     26032\n",
      "weighted avg   0.926231  0.924093  0.924318     26032\n",
      "-----------------------------------------------------\n",
      "Accuracy    Macro Average: 0.922040\n",
      "Accuracy F1-Macro Average: 0.917961\n",
      "\n",
      "\n",
      "Done (Tue Feb 18 18:52:55 2025)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeiks/Christian/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jeiks/Christian/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Finetune) Loading model from \"weights_primeiro/copycat_mnist.pth\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Testing ]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 814/814 [00:03<00:00, 221.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Macro F1-Score):\n",
      "  Oracle...: 0.936004\n",
      "  Copycat..: 0.917961\n",
      "  Finetune.: 0.090484\n",
      "Attack Performance\n",
      "  Copycat on Oracle...........: 98.07%\n",
      "  Finetuned Copycat on Oracle.: 9.67%\n",
      "Finished (Tue Feb 18 18:53:02 2025)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "options_mnist_svhn = Options(problem_name='SVHN',\n",
    "                   only_print_reports=None,\n",
    "                   config_file=None, validation_step=2, save_snapshot=1,\n",
    "                   #oracle\n",
    "                   oracle_filename='svhn.pth',\n",
    "                   oracle_resume_filename=None,\n",
    "                   oracle_arch='resnet18',\n",
    "                   train_oracle=True,\n",
    "                   oracle_max_epochs=oracle_epochs,\n",
    "                   oracle_batch_size=32,\n",
    "                   oracle_lr=0.0001,\n",
    "                   oracle_gamma=0.3,\n",
    "                   oracle_dataset_root='',\n",
    "                   #copycat\n",
    "                   copycat_filename='copycat_mnist_svhn.pth',\n",
    "                   copycat_resume_filename='weights_primeiro/copycat_mnist.pth',\n",
    "                   copycat_arch='resnet18',\n",
    "                   train_copycat=True,\n",
    "                   copycat_max_epochs=copycat_epochs,\n",
    "                   copycat_batch_size=32,\n",
    "                   copycat_lr=0.0001,\n",
    "                   copycat_gamma=0.3,\n",
    "                   copycat_dataset_root='',\n",
    "                   copycat_balance_dataset=None,\n",
    "                   label_copycat_dataset=True,\n",
    "                   #finetune\n",
    "                   finetune_filename='',\n",
    "                   finetune_resume_filename=None,\n",
    "                   train_finetune=False,\n",
    "                   finetune_max_epochs=None,\n",
    "                   finetune_batch_size=None,\n",
    "                   finetune_lr=None,\n",
    "                   finetune_gamma=None,\n",
    "                   finetune_dataset_root='',\n",
    "                   finetune_balance_dataset=None,\n",
    "                   label_finetune_dataset=False,\n",
    "                   #seed\n",
    "                   seed=seed)\n",
    "print(options_mnist_svhn)\n",
    "problem = Problem(problem=options_mnist_svhn.problem_name,\n",
    "                  #oracle\n",
    "                  oracle_arch=options_mnist_svhn.oracle_arch,\n",
    "                  oracle_filename=options_mnist_svhn.oracle_filename,\n",
    "                  oracle_resume_filename=options_mnist_svhn.oracle_resume_filename,\n",
    "                  oracle_dataset_root=options_mnist_svhn.oracle_dataset_root,\n",
    "                  #copycat\n",
    "                  copycat_arch=options_mnist_svhn.copycat_arch,\n",
    "                  copycat_filename=options_mnist_svhn.copycat_filename,\n",
    "                  copycat_resume_filename=options_mnist_svhn.copycat_resume_filename,\n",
    "                  copycat_dataset_root=options_mnist_svhn.copycat_dataset_root,\n",
    "                  #finetune\n",
    "                  finetune_filename=options_mnist_svhn.finetune_filename,\n",
    "                  finetune_resume_filename=options_mnist_svhn.finetune_resume_filename,\n",
    "                  finetune_dataset_root=options_mnist_svhn.finetune_dataset_root,\n",
    "                  #configuration file\n",
    "                  config_fn=options_mnist_svhn.config_file,\n",
    "                  #seed\n",
    "                  seed=options_mnist_svhn.seed)\n",
    "problem.run(validation_step=options_mnist.validation_step,\n",
    "                save_snapshot=options_mnist.save_snapshot,\n",
    "                #Oracle options:\n",
    "                train_oracle=options_mnist.train_oracle,\n",
    "                oracle_max_epochs=options_mnist.oracle_max_epochs,\n",
    "                oracle_batch_size=options_mnist.oracle_batch_size,\n",
    "                oracle_lr=options_mnist.oracle_lr,\n",
    "                oracle_gamma=options_mnist.oracle_gamma,\n",
    "                #Copycat options:\n",
    "                train_copycat=options_mnist.train_copycat,\n",
    "                label_copycat_dataset=options_mnist.label_copycat_dataset,\n",
    "                copycat_max_epochs=options_mnist.copycat_max_epochs,\n",
    "                copycat_batch_size=options_mnist.copycat_batch_size,\n",
    "                copycat_lr=options_mnist.copycat_lr,\n",
    "                copycat_gamma=options_mnist.copycat_gamma,\n",
    "                copycat_balance_dataset=options_mnist.copycat_balance_dataset,\n",
    "                #Copycat Finetune options:\n",
    "                finetune_copycat=options_mnist.train_finetune,\n",
    "                label_finetune_dataset=options_mnist.label_finetune_dataset,\n",
    "                finetune_max_epochs=options_mnist.finetune_max_epochs,\n",
    "                finetune_batch_size=options_mnist.finetune_batch_size,\n",
    "                finetune_lr=options_mnist.finetune_lr,\n",
    "                finetune_gamma=options_mnist.finetune_gamma,\n",
    "                finetune_balance_dataset=options_mnist.finetune_balance_dataset\n",
    "                 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
